{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ab2a53",
   "metadata": {},
   "source": [
    "### 1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9afee",
   "metadata": {},
   "source": [
    "One-hot encoding is a technique used in Natural Language Processing (NLP) to represent categorical data, such as words or characters, as numerical vectors. In this technique, a binary vector is created for each unique category, and each vector has the same length as the total number of unique categories.\n",
    "\n",
    "For example, suppose we have a corpus of text containing the following words: \"apple,\" \"banana,\" and \"orange.\" To represent these words using one-hot encoding, we would create a binary vector of length three for each word. The vector would contain a 1 in the position corresponding to the word, and zeros in all other positions.\n",
    "\n",
    "So, the one-hot encoding for \"apple\" would be [1,0,0], for \"banana\" it would be [0,1,0], and for \"orange\" it would be [0,0,1].\n",
    "\n",
    "One-hot encoding is commonly used in NLP for a variety of tasks, such as text classification and language generation, where a numerical representation of text is required. However, one limitation of one-hot encoding is that it can lead to a high-dimensional sparse feature space, which can be computationally expensive and may require dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd233e1",
   "metadata": {},
   "source": [
    "### 2. Explain Bag of Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a5469",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) is a popular technique used in Natural Language Processing (NLP) to represent textual data as a numerical vector. In BoW, a document is represented as a \"bag\" of its constituent words, without any consideration given to the order of the words or their grammatical structure.\n",
    "\n",
    "The BoW model consists of two steps:\n",
    "\n",
    "Tokenization: The text is first split into individual words, also called tokens. Punctuation and stopwords, which are words that are common and do not carry much meaning like \"the\", \"a\", \"an\", are typically removed during this process.\n",
    "Vectorization: Each token in the document is then counted, and a numerical vector is created, with each element representing the count of a particular token in the document.\n",
    "For example, suppose we have the following three documents:\n",
    "\n",
    "Document 1: \"I love pizza.\"\n",
    "Document 2: \"Pizza is my favorite food.\"\n",
    "Document 3: \"I eat pizza every day.\"\n",
    "We can represent these documents using the BoW model as follows:\n",
    "\n",
    "- I\tlove\tpizza\tis\tmy\tfavorite\tfood\teat\tevery\tday\n",
    "\n",
    "Document 1\t1\t1\t1\t0\t0\t0\t0\t0\t0\t0\n",
    "\n",
    "Document 2\t0\t0\t1\t1\t1\t1\t1\t0\t0\t0\n",
    "\n",
    "Document 3\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\n",
    "\n",
    "In this example, each column of the table represents a unique token in the documents, and the elements of the table represent the count of each token in each document.\n",
    "\n",
    "One limitation of BoW is that it ignores the context and order of the words in the document, which can result in a loss of information. Nevertheless, it remains a popular and effective technique for various NLP tasks, such as text classification and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6e5b6",
   "metadata": {},
   "source": [
    "### 3. Explain Bag of N-Grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98103646",
   "metadata": {},
   "source": [
    "Bag of N-Grams is a variation of the Bag of Words (BoW) model in Natural Language Processing (NLP), where instead of considering only single words as tokens, we consider contiguous sequences of n words, or n-grams, as tokens. The resulting model is also known as N-Gram Model.\n",
    "\n",
    "The N in N-Gram refers to the number of words in each sequence. For example, a bigram (N=2) is a sequence of two words, a trigram (N=3) is a sequence of three words, and so on.\n",
    "\n",
    "The Bag of N-Grams model works in a similar way to the BoW model, but instead of counting the frequency of individual words, it counts the frequency of n-grams in the document. This means that the vector representation of the document now includes not only single words but also sequences of adjacent words.\n",
    "\n",
    "For example, suppose we have the following sentence: \"I love to eat pizza.\"\n",
    "\n",
    "We can represent this sentence using a bigram model as follows:\n",
    "\n",
    "Bigrams: \"I love\", \"love to\", \"to eat\", \"eat pizza\"\n",
    "\n",
    "Vector representation: [1, 1, 1, 1]\n",
    "\n",
    "In this example, the vector represents the frequency of each bigram in the sentence.\n",
    "\n",
    "The Bag of N-Grams model can capture some of the contextual information of the document that is lost in the BoW model. For instance, bigrams can capture the context of words and their relationships to one another. However, this model can also suffer from the curse of dimensionality, especially with larger values of N, as the number of possible n-grams can grow exponentially with the length of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55136798",
   "metadata": {},
   "source": [
    "### 4. Explain TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de745733",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a popular technique in Natural Language Processing (NLP) used to weigh the importance of each term in a document relative to a corpus of documents. The idea behind TF-IDF is to penalize commonly occurring words and highlight words that are more distinctive or unique to a particular document.\n",
    "\n",
    "The TF-IDF model calculates a weight for each term in a document by combining two measures:\n",
    "\n",
    "Term Frequency (TF): This measure calculates the frequency of each term (word) in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in the document.\n",
    "\n",
    "TF = (Number of times the term appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "Inverse Document Frequency (IDF): This measure calculates the significance of a term in a corpus of documents. It is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents that contain the term.\n",
    "\n",
    "IDF = log (Total number of documents in the corpus / Number of documents that contain the term)\n",
    "\n",
    "The final weight of a term is obtained by multiplying its TF with its IDF.\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "The resulting weight reflects how important a term is to a document in the context of the corpus. If a term appears frequently in a document but rarely in other documents in the corpus, its weight will be high, indicating its significance in the document. Conversely, if a term appears frequently in all documents, its weight will be low, indicating its lack of uniqueness.\n",
    "\n",
    "The TF-IDF model is often used in tasks such as information retrieval, document classification, and clustering. It can help identify relevant documents or sentences in a corpus based on their content, and it can also reduce the impact of stop words that are common in a language but do not carry much meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edfc0c6",
   "metadata": {},
   "source": [
    "### 5. What is OOV problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e76a9",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), the OOV (Out-of-Vocabulary) problem refers to the situation where a word that is not present in the training vocabulary is encountered during the testing or inference phase.\n",
    "\n",
    "This problem arises because most NLP models, such as language models and classifiers, are trained on a fixed vocabulary of words. This vocabulary is usually created by collecting all the unique words that appear in the training corpus. However, there are always words that do not appear in the training corpus or are rare, and these words are not included in the vocabulary.\n",
    "\n",
    "When an OOV word is encountered during testing, it is typically replaced with a special token, such as \"<UNK>\" (unknown). This can lead to a loss of information and a reduction in the performance of the model, especially if the OOV words are important for the task at hand.\n",
    "\n",
    "Several techniques have been proposed to address the OOV problem in NLP, including sub-word tokenization, which breaks words into smaller units, allowing the model to recognize and generate words it has never seen before, and the use of pre-trained language models that have been trained on large amounts of data to recognize a wide range of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a78fe5",
   "metadata": {},
   "source": [
    "### 6. What are word embeddings?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377dbad",
   "metadata": {},
   "source": [
    "Word embeddings are a type of distributed representation used in Natural Language Processing (NLP) that maps words or phrases from a vocabulary to dense vectors of real numbers. Each word or phrase in the vocabulary is represented as a multi-dimensional vector, where each dimension represents a feature of the word or phrase, such as its semantic meaning or syntactic structure.\n",
    "\n",
    "Word embeddings are created by training a neural network on a large corpus of text, such as a collection of books or articles. The neural network learns to predict the context in which each word appears in the corpus, given a window of surrounding words. The weights of the neural network, which represent the learned relationships between words, are used as the word embeddings.\n",
    "\n",
    "The resulting word embeddings capture both the syntactic and semantic properties of words, allowing for more accurate and efficient analysis of text. For example, word embeddings can be used to determine the similarity between words based on their meaning, such as \"cat\" and \"dog\" being more similar than \"cat\" and \"car.\" They can also be used to perform tasks such as language translation, sentiment analysis, and named entity recognition.\n",
    "\n",
    "Some popular algorithms for generating word embeddings include Word2Vec, GloVe, and FastText. These algorithms have been shown to improve the performance of NLP models in a variety of tasks, and are widely used in industry and academia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fd478",
   "metadata": {},
   "source": [
    "### 7. Explain Continuous bag of words (CBOW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e1bc1",
   "metadata": {},
   "source": [
    "Continuous Bag of Words (CBOW) is a popular algorithm for generating word embeddings, a type of distributed representation used in Natural Language Processing (NLP). CBOW is a neural network model that learns to predict a target word from a window of surrounding words.\n",
    "\n",
    "In CBOW, the input layer of the neural network consists of a one-hot encoded vector representing the surrounding words. The input vectors are then multiplied by a weight matrix, which produces a hidden layer. The hidden layer is then multiplied by another weight matrix to produce the output layer, which represents the predicted target word.\n",
    "\n",
    "The objective of CBOW is to minimize the difference between the predicted target word and the actual target word. This is done by adjusting the weights of the neural network using backpropagation and stochastic gradient descent.\n",
    "\n",
    "During training, CBOW learns to produce word embeddings that capture the context in which each word appears in the training corpus. The resulting word embeddings are dense vectors of real numbers, where each dimension represents a feature of the word, such as its semantic meaning or syntactic structure.\n",
    "\n",
    "CBOW is often used for tasks such as language modeling, sentiment analysis, and text classification. It is generally faster to train than other models such as Skip-gram, which is another popular algorithm for generating word embeddings. However, CBOW may not perform as well as Skip-gram on tasks that require more fine-grained distinctions between words, such as analogy tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3a6ee",
   "metadata": {},
   "source": [
    "### 8. Explain SkipGram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b6afd",
   "metadata": {},
   "source": [
    "Skip-gram is a popular algorithm for generating word embeddings, a type of distributed representation used in Natural Language Processing (NLP). Unlike Continuous Bag of Words (CBOW), which predicts a target word from a window of surrounding words, Skip-gram predicts the surrounding words from a target word.\n",
    "\n",
    "In Skip-gram, the input layer of the neural network consists of a one-hot encoded vector representing the target word. The input vector is then multiplied by a weight matrix to produce a hidden layer. The hidden layer is then multiplied by another weight matrix to produce the output layer, which represents the predicted surrounding words.\n",
    "\n",
    "The objective of Skip-gram is to maximize the probability of the surrounding words given the target word. This is done by adjusting the weights of the neural network using backpropagation and stochastic gradient descent.\n",
    "\n",
    "During training, Skip-gram learns to produce word embeddings that capture the context in which each word appears in the training corpus. The resulting word embeddings are dense vectors of real numbers, where each dimension represents a feature of the word, such as its semantic meaning or syntactic structure.\n",
    "\n",
    "Skip-gram is often used for tasks such as language modeling, sentiment analysis, and text classification. It may perform better than CBOW on tasks that require more fine-grained distinctions between words, such as analogy tasks, because it explicitly models the relationships between a target word and its surrounding words.\n",
    "\n",
    "However, Skip-gram can be slower to train than CBOW because it requires training on more training examples. Additionally, Skip-gram may require more memory to store the resulting word embeddings because it produces a separate embedding for each word in the vocabulary, whereas CBOW produces a single embedding for each context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d790b2",
   "metadata": {},
   "source": [
    "### 9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6e7b3",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is an algorithm for generating word embeddings, a type of distributed representation used in Natural Language Processing (NLP). GloVe aims to learn word embeddings that capture both the co-occurrence statistics of words in a corpus and the global word co-occurrence statistics of the corpus.\n",
    "\n",
    "In GloVe, the algorithm constructs a co-occurrence matrix that represents the number of times each word appears in the context of every other word in the corpus. This matrix is then used to learn a set of word embeddings using a neural network. The objective of the neural network is to minimize the difference between the dot product of two word embeddings and the log of the co-occurrence count of the two corresponding words.\n",
    "\n",
    "The resulting word embeddings are dense vectors of real numbers, where each dimension represents a feature of the word, such as its semantic meaning or syntactic structure. The GloVe algorithm has been shown to produce high-quality word embeddings that outperform other algorithms such as Word2Vec and FastText on a variety of NLP tasks.\n",
    "\n",
    "GloVe embeddings have been pre-trained on large corpora such as Wikipedia and Common Crawl, and are widely used in industry and academia. They can be used as a starting point for training NLP models on specific tasks or fine-tuned on smaller corpora for better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
