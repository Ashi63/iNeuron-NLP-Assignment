{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba9dddc",
   "metadata": {},
   "source": [
    "### 1. What are Vanilla autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354870d3",
   "metadata": {},
   "source": [
    "Vanilla autoencoders, also known as traditional or basic autoencoders, are neural network models that are trained to reconstruct the input data by encoding the data into a lower-dimensional representation and then decoding it back into the original dimension. The encoding and decoding functions are implemented using neural networks, and the models are trained by minimizing the reconstruction error between the input and output data.\n",
    "\n",
    "Vanilla autoencoders have been used for various tasks such as dimensionality reduction, feature learning, and image denoising. However, they have some limitations such as being prone to overfitting, and they may not be able to capture complex patterns and correlations in the data. Therefore, various modifications and improvements have been proposed to address these issues, such as denoising autoencoders, variational autoencoders, and adversarial autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6624f59",
   "metadata": {},
   "source": [
    "### 2. What are Sparse autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef6bd9",
   "metadata": {},
   "source": [
    "Sparse autoencoders are a type of autoencoder neural network that learns a low-dimensional representation of the input data, while also enforcing sparsity in the learned representation. In a sparse autoencoder, the hidden layer is encouraged to have only a small number of neurons that are significantly active for a given input, while the remaining neurons are mostly inactive. This sparsity constraint can help the network learn more meaningful features and reduce the amount of redundancy in the learned representation.\n",
    "\n",
    "To enforce sparsity, a regularization term is added to the loss function of the autoencoder, which penalizes the activations of the hidden neurons that are not significantly active for a given input. This encourages the network to learn a compressed representation that contains only the most relevant features for the given input.\n",
    "\n",
    "Sparse autoencoders have been used in a variety of applications, including image and speech recognition, natural language processing, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3997b29",
   "metadata": {},
   "source": [
    "### 3. What are Denoising autoencoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03798d2",
   "metadata": {},
   "source": [
    "Denoising autoencoders (DAE) are a type of neural network that is trained to reconstruct clean data from corrupted data. The goal of a DAE is to learn a lower-dimensional representation of the input data while also being robust to noise and missing information in the input.\n",
    "\n",
    "The DAE works by first corrupting the input data by adding noise or removing some of the features. The corrupted input is then passed through an encoder neural network that maps it to a lower-dimensional latent space representation. The decoder neural network then takes this representation and reconstructs the original, uncorrupted input. The loss function used to train the DAE is based on the difference between the original input and the reconstructed output.\n",
    "\n",
    "The key idea behind DAE is that by forcing the autoencoder to reconstruct the original input from a corrupted version, it learns to extract the underlying structure of the data and ignore the noise. DAEs have been used in various applications such as image denoising, speech recognition, and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec67468",
   "metadata": {},
   "source": [
    "### 4. What are Convolutional autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fcc7e4",
   "metadata": {},
   "source": [
    "Convolutional Autoencoders (CAEs) are a type of autoencoder that use convolutional neural network (CNN) layers for encoding and decoding data. These types of autoencoders are especially useful for image-related tasks, such as image denoising, image compression, and image feature extraction.\n",
    "\n",
    "The encoding stage of the CAE consists of convolutional layers followed by pooling layers, which are used to downsample the input image. The decoding stage of the CAE consists of upsampling layers followed by deconvolutional layers, which are used to reconstruct the original image from the encoded representation.\n",
    "\n",
    "The CAE is trained to minimize the difference between the original image and the reconstructed image using a loss function such as mean squared error (MSE) or binary cross-entropy. By doing so, the CAE learns to extract useful features from the input image that can be used for various image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e91bf",
   "metadata": {},
   "source": [
    "### 5. What are Stacked autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036772d",
   "metadata": {},
   "source": [
    "Stacked autoencoders are a type of neural network architecture where multiple layers of autoencoders are stacked on top of each other to learn hierarchical representations of input data. The input data is first compressed into a lower-dimensional representation by an encoder, and then reconstructed back to the original shape by a decoder.\n",
    "\n",
    "In stacked autoencoders, the output of the encoder in one layer is used as the input to the encoder in the next layer. The entire stack can be trained together using backpropagation to optimize the weights in all layers simultaneously. This allows the network to learn complex and abstract features of the input data, as each layer can learn to represent higher-level features based on the output of the previous layer.\n",
    "\n",
    "Stacked autoencoders have been used for a variety of applications, such as image and speech recognition, natural language processing, and anomaly detection. They have shown to be particularly effective in learning useful representations for unsupervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c0ee2",
   "metadata": {},
   "source": [
    "### 6. Explain how to generate sentences using LSTM autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b973c51",
   "metadata": {},
   "source": [
    "LSTM autoencoders can be used to generate sentences by training the autoencoder on a corpus of text data, and then sampling from the encoded space to generate new sentences. Here are the general steps involved:\n",
    "\n",
    "1. Prepare the data: The first step is to prepare the data by tokenizing the text into a sequence of words, and then converting the words into a numerical representation, such as word embeddings or one-hot encoding.\n",
    "\n",
    "2. Train the autoencoder: Next, train the LSTM autoencoder on the prepared data. The autoencoder should have an encoder that compresses the input sequence into a lower-dimensional representation, and a decoder that reconstructs the input sequence from the compressed representation.\n",
    "\n",
    "3. Encode the input sentence: Once the autoencoder is trained, encode the input sentence by passing it through the encoder. This will produce a lower-dimensional representation of the sentence.\n",
    "\n",
    "4. Generate a new sentence: To generate a new sentence, sample a vector from a normal distribution and pass it through the decoder to reconstruct the sentence. The decoder will generate a sequence of words, which can be converted back into text.\n",
    "\n",
    "5. Repeat: To generate multiple sentences, repeat steps 3 and 4.\n",
    "\n",
    "Note that the quality of the generated sentences will depend on the quality of the autoencoder and the size and diversity of the training data. It's also important to balance the need for diversity in the generated sentences with the need for coherence and relevance to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66413e38",
   "metadata": {},
   "source": [
    "### 7. Explain Extractive summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c336a",
   "metadata": {},
   "source": [
    "Extractive summarization is a text summarization technique that involves selecting and extracting the most important sentences or phrases from a document to create a shorter summary that captures the main points of the original text. This is in contrast to abstractive summarization, where a summary is generated by paraphrasing and rewording the original text.\n",
    "\n",
    "In extractive summarization, the process typically involves:\n",
    "\n",
    "1. Text pre-processing: cleaning and preparing the text for summarization.\n",
    "2. Sentence scoring: assigning a score to each sentence based on its relevance and importance to the overall document.\n",
    "3. Sentence selection: selecting the top-scoring sentences to include in the summary.\n",
    "4. Summary generation: concatenating the selected sentences to create the final summary.\n",
    "\n",
    "The sentence scoring step can be done using a variety of techniques, including:\n",
    "\n",
    "- TextRank: a graph-based algorithm that scores sentences based on their importance in the overall document.\n",
    "- TF-IDF: a term frequency-inverse document frequency weighting scheme that scores sentences based on the importance of the words they contain.\n",
    "- Machine learning models: such as support vector machines or neural networks, which can be trained to predict the relevance of a sentence to the overall document.\n",
    "\n",
    "Extractive summarization is a popular technique for summarizing news articles, scientific papers, and other types of documents where the main points are clearly expressed in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae620e99",
   "metadata": {},
   "source": [
    "### 8. Explain Abstractive summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62cd3f",
   "metadata": {},
   "source": [
    "Abstractive summarization is a text summarization technique that involves generating a summary that is not an exact copy of the input text, but rather a shorter representation that captures the most important information from the original text. It involves understanding the meaning of the input text and generating new sentences to represent the important information in a more concise manner.\n",
    "\n",
    "Unlike extractive summarization, which selects sentences from the original text to form the summary, abstractive summarization involves creating a new summary that may contain new phrases and sentences that were not present in the original text. This requires the use of natural language processing techniques such as language modeling, neural machine translation, and paraphrasing to generate the summary.\n",
    "\n",
    "Abstractive summarization is a more challenging task than extractive summarization because it requires the model to understand the meaning of the text and generate new sentences that capture the important information in a coherent and concise manner. However, it can produce more accurate and concise summaries that capture the essence of the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f801c1",
   "metadata": {},
   "source": [
    "### 9. Explain Beam search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f504efa",
   "metadata": {},
   "source": [
    "Beam search is a search algorithm that is commonly used in natural language processing tasks such as machine translation, speech recognition, and text generation. It is a heuristic search algorithm that is used to find the most likely sequence of words that can be generated from a language model, given a certain input sequence.\n",
    "\n",
    "Beam search works by generating multiple candidate sequences at each step, keeping only the most likely ones, and then continuing to generate more candidate sequences from the most likely ones. The number of candidate sequences that are kept at each step is called the \"beam width\".\n",
    "\n",
    "For example, if the beam width is 5, then at each step the algorithm generates the 5 most likely candidate sequences, and then continues to generate more candidate sequences from those 5. The algorithm continues until it reaches the end of the sequence or a predefined stopping criterion.\n",
    "\n",
    "The advantage of using beam search is that it can improve the quality of the generated sequence by considering multiple candidate sequences, rather than just the single most likely one. However, the main disadvantage of beam search is that it can be computationally expensive, especially when the beam width is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4c531",
   "metadata": {},
   "source": [
    "### 10. Explain Length normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63ba82",
   "metadata": {},
   "source": [
    "Length normalization is a technique used in natural language processing to address the problem of sentence length bias in sequence generation tasks such as machine translation and text summarization. The length normalization approach adjusts the score of a generated sequence based on its length, so that longer sequences are not penalized and shorter sequences are not over-rewarded.\n",
    "\n",
    "One common length normalization technique is to divide the score of each generated sequence by a factor proportional to the length of the sequence raised to a certain power, typically between 0.5 and 1.0. For example, if the factor is set to the square root of the length of the sequence, the score of a sequence with length 4 would be divided by 2, while the score of a sequence with length 16 would be divided by 4. This way, longer sequences have a fair chance of being selected as the final output, regardless of their length.\n",
    "\n",
    "Length normalization is often used in conjunction with other techniques such as beam search to improve the performance of sequence generation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3d70f",
   "metadata": {},
   "source": [
    "### 11. Explain Coverage normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaffd38",
   "metadata": {},
   "source": [
    "Coverage normalization is a technique used in sequence-to-sequence models to ensure that the attention mechanism does not repeatedly attend to the same parts of the input sequence. This technique is particularly useful in abstractive summarization or machine translation, where the output sequence may be much shorter than the input sequence, and certain parts of the input may need to be emphasized more than others.\n",
    "\n",
    "In coverage normalization, the attention distribution is modified by a coverage vector, which is updated after each attention step. The coverage vector keeps track of how much attention has been paid to each part of the input sequence so far. At each step, the attention distribution is adjusted to reduce the weight given to the parts of the input sequence that have already been attended to. This encourages the model to attend to other parts of the sequence that have not yet been emphasized.\n",
    "\n",
    "Coverage normalization can be implemented by adding the coverage vector to the attention distribution, either before or after the softmax function is applied. The coverage vector can be updated using an element-wise addition of the attention distribution and the previous coverage vector. This updated coverage vector is then used in the next attention step to adjust the attention distribution.\n",
    "\n",
    "Coverage normalization has been shown to improve the performance of sequence-to-sequence models in various tasks, including abstractive summarization and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77ac72",
   "metadata": {},
   "source": [
    "### 12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f20e25",
   "metadata": {},
   "source": [
    "ROUGE stands for \"Recall-Oriented Understudy for Gisting Evaluation\". It is a set of metrics used for evaluating automatic summarization and machine translation systems.\n",
    "\n",
    "ROUGE measures the overlap between the generated summary or translation and the reference summary or translation. The overlap is computed based on n-gram matching between the two texts.\n",
    "\n",
    "ROUGE is a recall-based metric, which means it focuses on how much of the reference summary or translation is covered by the generated summary or translation. The main ROUGE metrics used are ROUGE-N, ROUGE-L, and ROUGE-W.\n",
    "\n",
    "ROUGE-N measures the n-gram overlap between the generated summary and the reference summary. ROUGE-L uses the longest common subsequence between the two texts, which takes into account the order of the words. ROUGE-W considers the matching of sequences of contiguous words rather than individual words.\n",
    "\n",
    "ROUGE scores range from 0 to 1, with a higher score indicating a better summary or translation. ROUGE is commonly used in research papers as a benchmark for summarization and translation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32834c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
